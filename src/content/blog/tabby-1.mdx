---
title: 'Tabby: Exploring Tabular Libraries Under The Hood | Part 1 - Data Structures and CSVs'
description: 'A small implementation of pyarrow/pandas/polars to better understand how they work and how they can be optimized.'
pubDate: 'May 10 2025'
heroImage: '/blog-placeholder-5.jpg'
---
import CodePreview from '../../components/CodePreview.astro';

## Introduction
I recently got my first data engineering job (Yay!).
During my first Jira ticket, I realized that we operate on large datasets.
Dataframes can approach the 100s of GBs, which is a lot more than many personal computers or VMs can handle.
At this scale, performance issues can have a serious increase in runtime.
It quickly became obvious that how the data is saved in a dataframe matters a lot too.
For example, one extra byte per row over hundreds of millions of rows just became hundreds of megabytes.
Therefore, the data type used but also the memory overhead are extremely important considerations.
I also realized that I don't much about how data science libraries like Pandas work.
I have taken filters, selects, joins, aggregations for granted but I don't really understand what does into making these work.
Of course, the algorithms used matter, but how can these operations be accelerated?
How do these work on large amounts of data?
For example, if the column of a dataframe is represented in a contiguous segment of memory and we perform a filter operation, then the column could become fragmented in memory.
Another puzzling use case is what happens when millions of rows are concatenated?
We can't just malloc megabytes, neither can we grow the current piece of memory.
We also don't want to copy everything into a new segment of memory as this is highly inefficient.
Understanding how such frameworks work is interesting since many concepts come into play, from algorithms to efficient memory usage, data oriented design, database design (because a dataframe is really just an in-memory database), and parallelism (using thread-level parallelism, parallel operations like SIMD, or GPUs).
The frameworks that I have come across so far are Pandas, the longstanding go-to, Pyarrow which is another alternative developed by Apache which is now a supported backend by Pandas, and Polars which is the new kid on the block that is written in Rust because Rust is the only acceptable progamming language at the moment.
In this article, I want to compare and explore how these frameworks represent data and execute common operations with performance in mind.
Some of the questions I have is how are millions of rows layed out in memory and how do the frameworks make it easy and efficient to add millions of rows more?
How do common operations like subset selection, filters, joins, concatenations, group by, and drop duplicates work?
How can these common operations be optimized on modern server hardware?

## Goal
In light of this, I want to implement the following operations:

- Reading from a file (I'll use CSVs for simplicity even though it's not the best format)
- Filter based on a boolean condition
- Join (left and inner) based on specified columns
- Concat (Append rows)
- Group by
- Drop duplicates

I'll implement the operations mentionned above in C++ with a Python wrapper.
I'll use Pybind11 for the wrapper because both a Reddit user and DeepSeek said so, which makes me confident it's the right way to go.
First, I'll implement all the operations in a basic, single-threaded fashion.
Then, I'll some optimizations techniques such as multithreading, SIMD, and perhaps CUDA too to see how much more speedâš¡ðŸš€ðŸ’¨ I can get.
I'll be using Kaggle's <i>Climate Weather Surface of Brazil - Hourly</i><sup>[1](#1)</sup> dataset for different benchmarks as it has a decent volume for testing on a personal machine.

## Data Structures
The first consideration about a tabular library is how it represents data in memory.
To illustrate the importance of good data structures and memory usage, I'll use an example.
Imagine you want to collect and analyze information about tweets.
You could want to collect data like the author's name and username, the date and time at which it was posted, the content of the tweet, the number of likes, the number of views, the number of comments, and the number of retweets.
Then, in an object-oriented fashion, you could model each tweet as such:
<CodePreview id='tweet-class-py'
code={
`
class Tweet:
    author_name: str
    author_username: str
    datetime: datetime.datetime
    text: str
    num_likes: int
    num_views: int
    num_comments: int
    num_retweets: int
`
}
language='python'>
</CodePreview>
Let's say you want to analyze the tweets over a day and for the given day, there were 100M tweets posted.
Each string in Python is represented as an object.
A Datetime is also an object.


### Pandas
The most commonly used data structure in Pandas is the DataFrame.
A DataFrame, according to the comments in the code, is a "two-dimensional, size-mutable, potentially heterogeneous tabular data [structure] [...] with labeled axes (rows and columns)".
It inherits from the NDFrame class, which is just like a DataFrame but n-dimensional.
A NDFrame mentions the use of a Manager class to manage and hold the data.
In the case of a DataFrame, the Manager is a BlockManager.
A BlockManager holds a sequence of blocks and each block contains a numpy ndarray.
From NumPy's documentation: "An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size".<sup>[1](#1)</sup>
This means that each block contains homogeneous data and that the data has a 2-D shape.
The BlockManager also holds a sequence of objects which are used to label the rows and columns.
Loading a sample dataset can help us see how Pandas uses blocks.

<CodePreview id='pandas-block'
code={
`import pandas as pd\n
northeast_pd = pd.read_csv('hourly-weather-surface-brazil-southeast-region/north.csv')
print(northeast_pd.shape)
print(northeast_pd._mgr.blocks)
print(northeast_pd._mgr.blocks[0].values[0].shape)
`
}
output={`(8392320, 27)\n(NumpyBlock: [ 3  4  5  6  8  9 10 11 12 13 18 19 24 25 26], 15 x 8392320, dtype: float64, NumpyBlock: [ 0  7 14 15 16 17], 6 x 8392320, dtype: int64, NumpyBlock: [ 1  2 20 21 22 23], 6 x 8392320, dtype: object)\n(8392320,)`}
language='python'>
</CodePreview>

We can see that the number of blocks for each data type corresponds to the number of columns with that data type.
Also, the shape of the values contained in one block is 1D.
In other words, Pandas uses 1 block per column and regroups them by data type.
So each column is represented by a NumPy ndarray.
This means that Pandas offloads a lot of the complexity to NumPy.
For instance, the problem of adding many rows is handled by NumPy.

### Pyarrow


### Polars
Polars is interesting as it has 2 main classes to represent frames: DataFrame and LazyFrame.
A LazyFrame is used to hold data and it supports lazy computations, or computations that occur only when they are needed.
This allows for optimizations to be made before actually running the computation.
In my case, I'll only look at the DataFrame to keep it simple and fair across all libraries.


## Project Setup
The first step is to create the CMake project and since I've never used Pybind11, I want to learn how to use it.
Creating a PyBind11 "Hello world" was surprisingly easy.

First, this must be added to the CMakeLists.txt
<code>
set(PYBIND11_FINDPYTHON ON)
find_package(pybind11 CONFIG REQUIRED)
</code>

Then, in C++, include PyBind11
<code>
`#include <pybind11/pybind11.h>`
</code>
and declare the PyBind11 module.
<code>
`PYBIND11_MODULE(tabby, m)`
`{`
   ` m.doc() = "pybind11 example plugin"; // optional module docstring`

    `m.def("add", &add, "A function that adds two numbers");`
`}`
</code>
Here, `add` is simply a function that adds two integers which is not worth showing.

After compilation, a shared object file is created.
When importing the module from Python, the shared object is automatically loaded without any other intervention from my part.
The function can now be called from Python.
It was surprisingly easy to use.

Here is the "hello world" example in Python.
The result printed is left as an exercise to the reader to determine.

<CodePreview id='import-tabby' fileName='test.py' code={`import tabby\nprint(tabby.add(2, 3))`} language='python' output='5'>
</CodePreview>




### References
<a id="1">[1]</a>
The N-dimensional array (ndarray). NumPy. Retrieved from https://numpy.org/doc/stable/reference/arrays.ndarray.html. (Accessed on May 30, 2025)
