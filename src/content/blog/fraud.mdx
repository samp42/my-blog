---
title: 'Fraud Detection'
description: 'Comparing different approaches to fraud detection in financial transactions.'
pubDate: 'Aug 27 2025'
heroImage: '/blog-placeholder-5.jpg'
---

import CodePreview from '../../components/CodePreview.astro';

## Introduction
My first job in the programming world was developing a web application in the cybersecurity world.
After I graduated, I moved to a different team.
This was my first professional exposure to data science.
I was tasked with translating extraction scripts from the SAS programming language to Python.

"After spending those initial months post-graduation focused on the foundational work of data engineering – cleaning, transforming, and structuring raw
information into usable formats for analytics – I felt a strong pull towards seeing that data come alive in predictive models.
It seemed like the logical next step to connect my backend skills with the front-end intelligence.

Fraud detection is one domain where this connection feels particularly vital.
Dealing with potentially massive datasets containing transaction details, user behaviour, and outcomes (often highly imbalanced), it's a prime candidate for ML application.
I've been exploring ways to get involved directly, and recently discovered Hugging Face's `datasets` tool as an excellent way to quickly load structured data like this.

So, combining my recent experience with the fundamentals of machine learning models (XGBoost, LightGBM known for gradient boosting efficiency; MLPs or
neural networks for capturing potentially complex patterns) and leveraging a clean dataset from Hugging Face, I decided to dive in myself. This project
isn't just about training models; it's also an opportunity to learn how different algorithms handle the specific nuances of fraud detection data – like its
inherent imbalance – something my engineering background helps frame but doesn't fully solve.

Join me as I explore these approaches on a real-world (though simplified via Hugging Face) financial dataset, comparing results and reflecting on what each
model brings to the table.

## The Dataset
The dataset I chose is the [Cifer Fraud Detection](https://huggingface.co/datasets/CiferAI/Cifer-Fraud-Detection-Dataset-AF) dataset available on Hugging Face.

It contains 21M rows which is quite substantial for training machine learning models.
The rows are separated in 14 parts which is intended to be use in federated learning but in my case I will just use it as a single dataset.
The data is high quality, exempt of null values, and structured with features representing transaction details, user behavior, and labels indicating fraudulent activity.
Here are the features as described on the dataset page:
| Feature Name | Description |
|---|---|
| step | Unit of time (1 step = 1 hour); simulation spans 30 days (744 steps total) |
| type | Transaction type: CASH-IN, CASH-OUT, DEBIT, PAYMENT, TRANSFER |
| amount | Transaction value in simulated currency |
| nameOrig | Anonymized ID of sender |
| oldbalanceOrig | Sender’s balance before transaction |
| newbalanceOrig | Sender’s balance after transaction |
| nameDest | Anonymized ID of recipient |
| oldbalanceDest | Recipient’s balance before transaction (if applicable) |
| newbalanceDest | Recipient’s balance after transaction (if applicable) |
| isFraud | Binary flag: 1 if transaction is fraudulent |
| isFlaggedFraud | 1 if transaction exceeds a flagged threshold (e.g. >200,000) |


Evaluating the distribution of classes is crucial in any machine learning task, especially in fraud detection where the class distribution is often highly imbalanced.
In this dataset, we can expect a significant disparity between the number of legitimate transactions and fraudulent ones.

To visualize this, we can create a bar chart showing the counts of each class in the `isFraud` column.
This will help us understand the extent of the imbalance and guide our approach to model training and evaluation.

![blog placeholder](../../../assets/fraud_detection_class_distribution.png)

We can see that the number of legitimate transactions far outweighs the number of fraudulent ones, highlighting the class imbalance issue we need to address.
In fact, 99.8% of the transactions are legitimate, while only 0.2% are fraudulent.
We will have to employ techniques such as resampling, cost-sensitive learning, or anomaly detection to effectively train our models on this imbalanced dataset.

To mitigate this, I'll experiment with two approaches.
First, I'll use undersampling to reduce the number of legitimate transactions.
This involves randomly removing a portion of the legitimate transactions to create a more balanced dataset.
The advantage of this approach is that it can help the model focus on the minority class (fraudulent transactions) without being overwhelmed by the majority class (legitimate transactions).
It also does not require duplicating the minority class, which can lead to overfitting.
With this dataset, I'm lucky to be able to afford to lose some data from the majority class without significantly impacting the model's ability to learn.
In fact, there are 27,470 fraudulent transactions which is a manageable number for training.
Second, I'll use cost-sensitive learning to give more importance to the minority class during training.
This involves adjusting the loss function to penalize misclassifications of the minority class more than the majority class.
By doing this, the model will learn to pay more attention to the fraudulent transactions, which can improve its performance on this imbalanced dataset.
More approaches are possible to deal with imbalanced datasets, but these two will be sufficient for this exploration.

## Data Preparation
As we saw in the previous section, the dataset as a few columns with relevant features.
The `type` column represents a categorical feature which will need to be encoded.
I will use one-hot encoding since there are few values and it won't introduce an order or a bigger magnitude to any category.
As for the features present in the raw dataset, this is a great starting point, but these features alone would likely not allow for a model that learns patterns more
intricate then wether a transaction is of a certain type and involves a big amount of money.
We need to enhance the dataset with some extra features.
While I don't know what makes a transaction fraudulent, I have some clues that could be interesting to test.
My approach will be to add features that I think could be relevant and let the models figure out which are and which are not.
It will be interesting to see in the end what actually matters.
Here are some of the features that I will add to the dataset:
- Z-score for each transaction of a sender indicating how far from the average transaction of a specific sender it is
- Z-score for each transaction of a receiver
- Number of transactions of the same amount in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Number of transactions by a sender in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Number of transactions by a receiver in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Number of transactions an individual is involved in in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Number of unique receivers per sender in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Number of unique senders per receiver in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Total amount received by an individual in a single step, 24 steps (1 day), 168 steps (1 week), all steps
- Wether a sender has had a flagged transaction before
- Wether a receiver has had a flagged transaction before
- Wether a sender is also a receiver

I decided to create most of the features based on 1 hour, 1 day, 1 week, and 1 month to hopefully detect some patterns in time.
The choice of the last feature, namely wether a sender is also a receiver, comes from the intuition that maybe a fraudster only receives money from his account.
This will make an extensive feature set that will, I hope, capture most if not all of the possible strategies.
